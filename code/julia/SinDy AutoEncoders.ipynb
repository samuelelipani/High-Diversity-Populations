{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d51977a3",
   "metadata": {},
   "source": [
    "<!-- ![image.png](attachment:image.png, width=\"200\"} -->\n",
    "<img src=\"Sindy AE.png\" alt=\"Sindy AutoEncoder Model\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84582d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ContextVariablesX ────── v0.1.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ShowCases ────────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Optimisers ───────────── v0.2.18\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NNlib ────────────────── v0.8.21\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m InitialValues ────────── v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m TimerOutputs ─────────── v0.5.23\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUArrays ────────────── v8.8.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m cuDNN ────────────────── v1.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NNlibCUDA ────────────── v0.2.7\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BFloat16s ────────────── v0.4.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDA_Runtime_jll ─────── v0.6.0+0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLUtils ──────────────── v0.4.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PrettyPrint ──────────── v0.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m RealDot ──────────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IRTools ──────────────── v0.4.10\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LLVM ─────────────────── v6.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Functors ─────────────── v0.4.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FLoopsBase ───────────── v0.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ProgressLogging ──────── v0.1.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDA_Driver_jll ──────── v0.5.0+1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OneHotArrays ─────────── v0.2.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MicroCollections ─────── v0.1.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDA_Runtime_Discovery ─ v0.2.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NameResolution ───────── v0.1.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DefineSingletons ─────── v0.1.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Zygote ───────────────── v0.6.62\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m UnsafeAtomicsLLVM ────── v0.1.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRules ───────────── v1.52.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLStyle ──────────────── v0.4.17\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BangBang ─────────────── v0.3.39\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ArgCheck ─────────────── v2.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FLoops ───────────────── v0.2.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AbstractFFTs ─────────── v1.4.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LLVMExtra_jll ────────── v0.0.23+0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m KernelAbstractions ───── v0.9.6\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Flux ─────────────────── v0.13.17\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CompositionsBase ─────── v0.1.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JuliaVariables ───────── v0.2.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m UnsafeAtomics ────────── v0.2.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Atomix ───────────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Transducers ──────────── v0.4.77\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Baselet ──────────────── v0.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUCompiler ──────────── v0.21.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StructArrays ─────────── v0.6.15\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SplittablesBase ──────── v0.1.15\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDNN_jll ────────────── v8.9.2+0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDA ─────────────────── v4.4.0\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Micha\\.julia\\environments\\v1.9\\Project.toml`\n",
      "  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.13.17\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Micha\\.julia\\environments\\v1.9\\Manifest.toml`\n",
      " Downloading artifact: CUDA_Runtime\n",
      "  \u001b[90m[621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.4.0\u001b[39m\n",
      "  \u001b[90m[dce04be8] \u001b[39m\u001b[92m+ ArgCheck v2.3.0\u001b[39m\n",
      "  \u001b[90m[a9b6321e] \u001b[39m\u001b[92m+ Atomix v0.1.0\u001b[39m\n",
      "  \u001b[90m[ab4f0b2a] \u001b[39m\u001b[92m+ BFloat16s v0.4.2\u001b[39m\n",
      "  \u001b[90m[198e06fe] \u001b[39m\u001b[92m+ BangBang v0.3.39\u001b[39m\n",
      "  \u001b[90m[9718e550] \u001b[39m\u001b[92m+ Baselet v0.1.1\u001b[39m\n",
      "  \u001b[90m[052768ef] \u001b[39m\u001b[92m+ CUDA v4.4.0\u001b[39m\n",
      "  \u001b[90m[1af6417a] \u001b[39m\u001b[92m+ CUDA_Runtime_Discovery v0.2.2\u001b[39m\n",
      "  \u001b[90m[082447d4] \u001b[39m\u001b[92m+ ChainRules v1.52.0\u001b[39m\n",
      "  \u001b[90m[a33af91c] \u001b[39m\u001b[92m+ CompositionsBase v0.1.2\u001b[39m\n",
      "  \u001b[90m[6add18c4] \u001b[39m\u001b[92m+ ContextVariablesX v0.1.3\u001b[39m\n",
      "  \u001b[90m[244e2a9f] \u001b[39m\u001b[92m+ DefineSingletons v0.1.2\u001b[39m\n",
      "  \u001b[90m[cc61a311] \u001b[39m\u001b[92m+ FLoops v0.2.1\u001b[39m\n",
      "  \u001b[90m[b9860ae5] \u001b[39m\u001b[92m+ FLoopsBase v0.1.1\u001b[39m\n",
      "  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.13.17\u001b[39m\n",
      "  \u001b[90m[d9f16b24] \u001b[39m\u001b[92m+ Functors v0.4.5\u001b[39m\n",
      "  \u001b[90m[0c68f7d7] \u001b[39m\u001b[92m+ GPUArrays v8.8.1\u001b[39m\n",
      "  \u001b[90m[61eb1bfa] \u001b[39m\u001b[92m+ GPUCompiler v0.21.3\u001b[39m\n",
      "  \u001b[90m[7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.10\u001b[39m\n",
      "  \u001b[90m[22cec73e] \u001b[39m\u001b[92m+ InitialValues v0.3.1\u001b[39m\n",
      "  \u001b[90m[b14d175d] \u001b[39m\u001b[92m+ JuliaVariables v0.2.4\u001b[39m\n",
      "  \u001b[90m[63c18a36] \u001b[39m\u001b[92m+ KernelAbstractions v0.9.6\u001b[39m\n",
      "  \u001b[90m[929cbde3] \u001b[39m\u001b[92m+ LLVM v6.1.0\u001b[39m\n",
      "  \u001b[90m[d8e11817] \u001b[39m\u001b[92m+ MLStyle v0.4.17\u001b[39m\n",
      "  \u001b[90m[f1d291b0] \u001b[39m\u001b[92m+ MLUtils v0.4.3\u001b[39m\n",
      "  \u001b[90m[128add7d] \u001b[39m\u001b[92m+ MicroCollections v0.1.4\u001b[39m\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[872c559c] \u001b[39m\u001b[92m+ NNlib v0.8.21\u001b[39m\n",
      "  \u001b[90m[a00861dc] \u001b[39m\u001b[92m+ NNlibCUDA v0.2.7\u001b[39m\n",
      "  \u001b[90m[71a1bf82] \u001b[39m\u001b[92m+ NameResolution v0.1.5\u001b[39m\n",
      "  \u001b[90m[0b1bfda6] \u001b[39m\u001b[92m+ OneHotArrays v0.2.4\u001b[39m\n",
      "  \u001b[90m[3bd65402] \u001b[39m\u001b[92m+ Optimisers v0.2.18\u001b[39m\n",
      "  \u001b[90m[8162dcfd] \u001b[39m\u001b[92m+ PrettyPrint v0.2.0\u001b[39m\n",
      "  \u001b[90m[33c8b6b6] \u001b[39m\u001b[92m+ ProgressLogging v0.1.4\u001b[39m\n",
      "  \u001b[90m[c1ae055f] \u001b[39m\u001b[92m+ RealDot v0.1.0\u001b[39m\n",
      "  \u001b[90m[605ecd9f] \u001b[39m\u001b[92m+ ShowCases v0.1.0\u001b[39m\n",
      "  \u001b[90m[171d559e] \u001b[39m\u001b[92m+ SplittablesBase v0.1.15\u001b[39m\n",
      "  \u001b[90m[09ab397b] \u001b[39m\u001b[92m+ StructArrays v0.6.15\u001b[39m\n",
      "  \u001b[90m[a759f4b9] \u001b[39m\u001b[92m+ TimerOutputs v0.5.23\u001b[39m\n",
      "  \u001b[90m[28d57a85] \u001b[39m\u001b[92m+ Transducers v0.4.77\u001b[39m\n",
      "  \u001b[90m[013be700] \u001b[39m\u001b[92m+ UnsafeAtomics v0.2.1\u001b[39m\n",
      "  \u001b[90m[d80eeb9a] \u001b[39m\u001b[92m+ UnsafeAtomicsLLVM v0.1.3\u001b[39m\n",
      "  \u001b[90m[e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.6.62\u001b[39m\n",
      "  \u001b[90m[02a925ec] \u001b[39m\u001b[92m+ cuDNN v1.1.0\u001b[39m\n",
      "  \u001b[90m[4ee394cb] \u001b[39m\u001b[92m+ CUDA_Driver_jll v0.5.0+1\u001b[39m\n",
      "\u001b[91m→\u001b[39m \u001b[90m[76a88914] \u001b[39m\u001b[92m+ CUDA_Runtime_jll v0.6.0+0\u001b[39m\n",
      "\u001b[91m→\u001b[39m \u001b[90m[62b44479] \u001b[39m\u001b[92m+ CUDNN_jll v8.9.2+0\u001b[39m\n",
      "  \u001b[90m[dad2f222] \u001b[39m\u001b[92m+ LLVMExtra_jll v0.0.23+0\u001b[39m\n",
      "  \u001b[90m[4af54fe1] \u001b[39m\u001b[92m+ LazyArtifacts\u001b[39m\n",
      "\u001b[36m\u001b[1m        Info\u001b[22m\u001b[39m Packages marked with \u001b[91m→\u001b[39m are not downloaded, use `instantiate` to download\n",
      "\u001b[36m\u001b[1m        Info\u001b[22m\u001b[39m Packages marked with \u001b[33m⌅\u001b[39m have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated -m`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Flux\")\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bde5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca76451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bed375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import binom\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "\n",
    "def library_size(n, poly_order, use_sine=False, include_constant=True):\n",
    "    l = 0\n",
    "    for k in range(poly_order+1):\n",
    "        l += int(binom(n+k-1,k))\n",
    "    if use_sine:\n",
    "        l += n\n",
    "    if not include_constant:\n",
    "        l -= 1\n",
    "    return l\n",
    "\n",
    "\n",
    "def sindy_library(X, poly_order, include_sine=False):\n",
    "    m,n = X.shape\n",
    "    l = library_size(n, poly_order, include_sine, True)\n",
    "    library = np.ones((m,l))\n",
    "    index = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        library[:,index] = X[:,i]\n",
    "        index += 1\n",
    "\n",
    "    if poly_order > 1:\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                library[:,index] = X[:,i]*X[:,j]\n",
    "                index += 1\n",
    "\n",
    "    if poly_order > 2:\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                for k in range(j,n):\n",
    "                    library[:,index] = X[:,i]*X[:,j]*X[:,k]\n",
    "                    index += 1\n",
    "\n",
    "    if poly_order > 3:\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                for k in range(j,n):\n",
    "                    for q in range(k,n):\n",
    "                        library[:,index] = X[:,i]*X[:,j]*X[:,k]*X[:,q]\n",
    "                        index += 1\n",
    "                    \n",
    "    if poly_order > 4:\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                for k in range(j,n):\n",
    "                    for q in range(k,n):\n",
    "                        for r in range(q,n):\n",
    "                            library[:,index] = X[:,i]*X[:,j]*X[:,k]*X[:,q]*X[:,r]\n",
    "                            index += 1\n",
    "\n",
    "    if include_sine:\n",
    "        for i in range(n):\n",
    "            library[:,index] = np.sin(X[:,i])\n",
    "            index += 1\n",
    "\n",
    "    return library\n",
    "\n",
    "\n",
    "def sindy_library_order2(X, dX, poly_order, include_sine=False):\n",
    "    m,n = X.shape\n",
    "    l = library_size(2*n, poly_order, include_sine, True)\n",
    "    library = np.ones((m,l))\n",
    "    index = 1\n",
    "\n",
    "    X_combined = np.concatenate((X, dX), axis=1)\n",
    "\n",
    "    for i in range(2*n):\n",
    "        library[:,index] = X_combined[:,i]\n",
    "        index += 1\n",
    "\n",
    "    if poly_order > 1:\n",
    "        for i in range(2*n):\n",
    "            for j in range(i,2*n):\n",
    "                library[:,index] = X_combined[:,i]*X_combined[:,j]\n",
    "                index += 1\n",
    "\n",
    "    if poly_order > 2:\n",
    "        for i in range(2*n):\n",
    "            for j in range(i,2*n):\n",
    "                for k in range(j,2*n):\n",
    "                    library[:,index] = X_combined[:,i]*X_combined[:,j]*X_combined[:,k]\n",
    "                    index += 1\n",
    "\n",
    "    if poly_order > 3:\n",
    "        for i in range(2*n):\n",
    "            for j in range(i,2*n):\n",
    "                for k in range(j,2*n):\n",
    "                    for q in range(k,2*n):\n",
    "                        library[:,index] = X_combined[:,i]*X_combined[:,j]*X_combined[:,k]*X_combined[:,q]\n",
    "                        index += 1\n",
    "                    \n",
    "    if poly_order > 4:\n",
    "        for i in range(2*n):\n",
    "            for j in range(i,2*n):\n",
    "                for k in range(j,2*n):\n",
    "                    for q in range(k,2*n):\n",
    "                        for r in range(q,2*n):\n",
    "                            library[:,index] = X_combined[:,i]*X_combined[:,j]*X_combined[:,k]*X_combined[:,q]*X_combined[:,r]\n",
    "                            index += 1\n",
    "\n",
    "    if include_sine:\n",
    "        for i in range(2*n):\n",
    "            library[:,index] = np.sin(X_combined[:,i])\n",
    "            index += 1\n",
    "\n",
    "    return library\n",
    "\n",
    "\n",
    "def sindy_fit(RHS, LHS, coefficient_threshold):\n",
    "    m,n = LHS.shape\n",
    "    Xi = np.linalg.lstsq(RHS,LHS, rcond=None)[0]\n",
    "    \n",
    "    for k in range(10):\n",
    "        small_inds = (np.abs(Xi) < coefficient_threshold)\n",
    "        Xi[small_inds] = 0\n",
    "        for i in range(n):\n",
    "            big_inds = ~small_inds[:,i]\n",
    "            if np.where(big_inds)[0].size == 0:\n",
    "                continue\n",
    "            Xi[big_inds,i] = np.linalg.lstsq(RHS[:,big_inds], LHS[:,i], rcond=None)[0]\n",
    "    return Xi\n",
    "\n",
    "\n",
    "def sindy_simulate(x0, t, Xi, poly_order, include_sine):\n",
    "    m = t.size\n",
    "    n = x0.size\n",
    "    f = lambda x,t : np.dot(sindy_library(np.array(x).reshape((1,n)), poly_order, include_sine), Xi).reshape((n,))\n",
    "\n",
    "    x = odeint(f, x0, t)\n",
    "    return x\n",
    "\n",
    "\n",
    "def sindy_simulate_order2(x0, dx0, t, Xi, poly_order, include_sine):\n",
    "    m = t.size\n",
    "    n = 2*x0.size\n",
    "    l = Xi.shape[0]\n",
    "\n",
    "    Xi_order1 = np.zeros((l,n))\n",
    "    for i in range(n//2):\n",
    "        Xi_order1[2*(i+1),i] = 1.\n",
    "        Xi_order1[:,i+n//2] = Xi[:,i]\n",
    "    \n",
    "    x = sindy_simulate(np.concatenate((x0,dx0)), t, Xi_order1, poly_order, include_sine)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def full_network(params):\n",
    "    \"\"\"\n",
    "    Define the full network architecture.\n",
    "\n",
    "    Arguments:\n",
    "        params - Dictionary object containing the parameters that specify the training.\n",
    "        See README file for a description of the parameters.\n",
    "\n",
    "    Returns:\n",
    "        network - Dictionary containing the tensorflow objects that make up the network.\n",
    "    \"\"\"\n",
    "    input_dim = params['input_dim']\n",
    "    latent_dim = params['latent_dim']\n",
    "    activation = params['activation']\n",
    "    poly_order = params['poly_order']\n",
    "    if 'include_sine' in params.keys():\n",
    "        include_sine = params['include_sine']\n",
    "    else:\n",
    "        include_sine = False\n",
    "    library_dim = params['library_dim']\n",
    "    model_order = params['model_order']\n",
    "\n",
    "    network = {}\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, input_dim], name='x')\n",
    "    dx = tf.placeholder(tf.float32, shape=[None, input_dim], name='dx')\n",
    "    if model_order == 2:\n",
    "        ddx = tf.placeholder(tf.float32, shape=[None, input_dim], name='ddx')\n",
    "\n",
    "    if activation == 'linear':\n",
    "        z, x_decode, encoder_weights, encoder_biases, decoder_weights, decoder_biases = linear_autoencoder(x, input_dim, latent_dim)\n",
    "    else:\n",
    "        z, x_decode, encoder_weights, encoder_biases, decoder_weights, decoder_biases = nonlinear_autoencoder(x, input_dim, latent_dim, params['widths'], activation=activation)\n",
    "    \n",
    "    if model_order == 1:\n",
    "        dz = z_derivative(x, dx, encoder_weights, encoder_biases, activation=activation)\n",
    "        Theta = sindy_library_tf(z, latent_dim, poly_order, include_sine)\n",
    "    else:\n",
    "        dz,ddz = z_derivative_order2(x, dx, ddx, encoder_weights, encoder_biases, activation=activation)\n",
    "        Theta = sindy_library_tf_order2(z, dz, latent_dim, poly_order, include_sine)\n",
    "\n",
    "    if params['coefficient_initialization'] == 'xavier':\n",
    "        sindy_coefficients = tf.get_variable('sindy_coefficients', shape=[library_dim,latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    elif params['coefficient_initialization'] == 'specified':\n",
    "        sindy_coefficients = tf.get_variable('sindy_coefficients', initializer=params['init_coefficients'])\n",
    "    elif params['coefficient_initialization'] == 'constant':\n",
    "        sindy_coefficients = tf.get_variable('sindy_coefficients', shape=[library_dim,latent_dim], initializer=tf.constant_initializer(1.0))\n",
    "    elif params['coefficient_initialization'] == 'normal':\n",
    "        sindy_coefficients = tf.get_variable('sindy_coefficients', shape=[library_dim,latent_dim], initializer=tf.initializers.random_normal())\n",
    "    \n",
    "    if params['sequential_thresholding']:\n",
    "        coefficient_mask = tf.placeholder(tf.float32, shape=[library_dim,latent_dim], name='coefficient_mask')\n",
    "        sindy_predict = tf.matmul(Theta, coefficient_mask*sindy_coefficients)\n",
    "        network['coefficient_mask'] = coefficient_mask\n",
    "    else:\n",
    "        sindy_predict = tf.matmul(Theta, sindy_coefficients)\n",
    "\n",
    "    if model_order == 1:\n",
    "        dx_decode = z_derivative(z, sindy_predict, decoder_weights, decoder_biases, activation=activation)\n",
    "    else:\n",
    "        dx_decode,ddx_decode = z_derivative_order2(z, dz, sindy_predict, decoder_weights, decoder_biases,\n",
    "                                             activation=activation)\n",
    "\n",
    "    network['x'] = x\n",
    "    network['dx'] = dx\n",
    "    network['z'] = z\n",
    "    network['dz'] = dz\n",
    "    network['x_decode'] = x_decode\n",
    "    network['dx_decode'] = dx_decode\n",
    "    network['encoder_weights'] = encoder_weights\n",
    "    network['encoder_biases'] = encoder_biases\n",
    "    network['decoder_weights'] = decoder_weights\n",
    "    network['decoder_biases'] = decoder_biases\n",
    "    network['Theta'] = Theta\n",
    "    network['sindy_coefficients'] = sindy_coefficients\n",
    "\n",
    "    if model_order == 1:\n",
    "        network['dz_predict'] = sindy_predict\n",
    "    else:\n",
    "        network['ddz'] = ddz\n",
    "        network['ddz_predict'] = sindy_predict\n",
    "        network['ddx'] = ddx\n",
    "        network['ddx_decode'] = ddx_decode\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def define_loss(network, params):\n",
    "    \"\"\"\n",
    "    Create the loss functions.\n",
    "\n",
    "    Arguments:\n",
    "        network - Dictionary object containing the elements of the network architecture.\n",
    "        This will be the output of the full_network() function.\n",
    "    \"\"\"\n",
    "    x = network['x']\n",
    "    x_decode = network['x_decode']\n",
    "    if params['model_order'] == 1:\n",
    "        dz = network['dz']\n",
    "        dz_predict = network['dz_predict']\n",
    "        dx = network['dx']\n",
    "        dx_decode = network['dx_decode']\n",
    "    else:\n",
    "        ddz = network['ddz']\n",
    "        ddz_predict = network['ddz_predict']\n",
    "        ddx = network['ddx']\n",
    "        ddx_decode = network['ddx_decode']\n",
    "    sindy_coefficients = params['coefficient_mask']*network['sindy_coefficients']\n",
    "\n",
    "    losses = {}\n",
    "    losses['decoder'] = tf.reduce_mean((x - x_decode)**2)\n",
    "    if params['model_order'] == 1:\n",
    "        losses['sindy_z'] = tf.reduce_mean((dz - dz_predict)**2)\n",
    "        losses['sindy_x'] = tf.reduce_mean((dx - dx_decode)**2)\n",
    "    else:\n",
    "        losses['sindy_z'] = tf.reduce_mean((ddz - ddz_predict)**2)\n",
    "        losses['sindy_x'] = tf.reduce_mean((ddx - ddx_decode)**2)\n",
    "    losses['sindy_regularization'] = tf.reduce_mean(tf.abs(sindy_coefficients))\n",
    "    loss = params['loss_weight_decoder'] * losses['decoder'] \\\n",
    "           + params['loss_weight_sindy_z'] * losses['sindy_z'] \\\n",
    "           + params['loss_weight_sindy_x'] * losses['sindy_x'] \\\n",
    "           + params['loss_weight_sindy_regularization'] * losses['sindy_regularization']\n",
    "\n",
    "    loss_refinement = params['loss_weight_decoder'] * losses['decoder'] \\\n",
    "                      + params['loss_weight_sindy_z'] * losses['sindy_z'] \\\n",
    "                      + params['loss_weight_sindy_x'] * losses['sindy_x']\n",
    "\n",
    "    return loss, losses, loss_refinement\n",
    "\n",
    "\n",
    "def linear_autoencoder(x, input_dim, d):\n",
    "    # z,encoder_weights,encoder_biases = encoder(x, input_dim, latent_dim, [], None, 'encoder')\n",
    "    # x_decode,decoder_weights,decoder_biases = decoder(z, input_dim, latent_dim, [], None, 'decoder')\n",
    "    z,encoder_weights,encoder_biases = build_network_layers(x, input_dim, latent_dim, [], None, 'encoder')\n",
    "    x_decode,decoder_weights,decoder_biases = build_network_layers(z, latent_dim, input_dim, [], None, 'decoder')\n",
    "\n",
    "    return z, x_decode, encoder_weights, encoder_biases,decoder_weights,decoder_biases\n",
    "\n",
    "\n",
    "def nonlinear_autoencoder(x, input_dim, latent_dim, widths, activation='elu'):\n",
    "    \"\"\"\n",
    "    Construct a nonlinear autoencoder.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Returns:\n",
    "        z -\n",
    "        x_decode -\n",
    "        encoder_weights - List of tensorflow arrays containing the encoder weights\n",
    "        encoder_biases - List of tensorflow arrays containing the encoder biases\n",
    "        decoder_weights - List of tensorflow arrays containing the decoder weights\n",
    "        decoder_biases - List of tensorflow arrays containing the decoder biases\n",
    "    \"\"\"\n",
    "    if activation == 'relu':\n",
    "        activation_function = tf.nn.relu\n",
    "    elif activation == 'elu':\n",
    "        activation_function = tf.nn.elu\n",
    "    elif activation == 'sigmoid':\n",
    "        activation_function = tf.sigmoid\n",
    "    else:\n",
    "        raise ValueError('invalid activation function')\n",
    "    # z,encoder_weights,encoder_biases = encoder(x, input_dim, latent_dim, widths, activation_function, 'encoder')\n",
    "    # x_decode,decoder_weights,decoder_biases = decoder(z, input_dim, latent_dim, widths[::-1], activation_function, 'decoder')\n",
    "    z,encoder_weights,encoder_biases = build_network_layers(x, input_dim, latent_dim, widths, activation_function, 'encoder')\n",
    "    x_decode,decoder_weights,decoder_biases = build_network_layers(z, latent_dim, input_dim, widths[::-1], activation_function, 'decoder')\n",
    "\n",
    "    return z, x_decode, encoder_weights, encoder_biases, decoder_weights, decoder_biases\n",
    "\n",
    "\n",
    "def build_network_layers(input, input_dim, output_dim, widths, activation, name):\n",
    "    \"\"\"\n",
    "    Construct one portion of the network (either encoder or decoder).\n",
    "\n",
    "    Arguments:\n",
    "        input - 2D tensorflow array, input to the network (shape is [?,input_dim])\n",
    "        input_dim - Integer, number of state variables in the input to the first layer\n",
    "        output_dim - Integer, number of state variables to output from the final layer\n",
    "        widths - List of integers representing how many units are in each network layer\n",
    "        activation - Tensorflow function to be used as the activation function at each layer\n",
    "        name - String, prefix to be used in naming the tensorflow variables\n",
    "\n",
    "    Returns:\n",
    "        input - Tensorflow array, output of the network layers (shape is [?,output_dim])\n",
    "        weights - List of tensorflow arrays containing the network weights\n",
    "        biases - List of tensorflow arrays containing the network biases\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases = []\n",
    "    last_width=input_dim\n",
    "    for i,n_units in enumerate(widths):\n",
    "        W = tf.get_variable(name+'_W'+str(i), shape=[last_width,n_units],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(name+'_b'+str(i), shape=[n_units],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "        input = tf.matmul(input, W) + b\n",
    "        if activation is not None:\n",
    "            input = activation(input)\n",
    "        last_width = n_units\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    W = tf.get_variable(name+'_W'+str(len(widths)), shape=[last_width,output_dim],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(name+'_b'+str(len(widths)), shape=[output_dim],\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    input = tf.matmul(input,W) + b\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    return input, weights, biases\n",
    "\n",
    "\n",
    "# def encoder(input, input_dim, d, widths, activation, name):\n",
    "#     \"\"\"\n",
    "#     Construct the encoder.\n",
    "\n",
    "#     Arguments:\n",
    "#         input - 2D tensorflow array, input to the network (shape is [?,input_dim])\n",
    "#         input_dim - Integer, number of state variables in the original data space\n",
    "#         d - Integer, number of state variables in the decoder space\n",
    "#         widths - List of integers representing how many units are in each network layer\n",
    "#         activation - Tensorflow function to be used as the activation function at each layer\n",
    "#         name - String, prefix to be used in naming the tensorflow variables\n",
    "\n",
    "#     Returns:\n",
    "#         input - Tensorflow array, output of the encoder (shape is [?,d])\n",
    "#         weights - List of tensorflow arrays containing the network weights\n",
    "#         biases - List of tensorflow arrays containing the network biases\n",
    "#     \"\"\"\n",
    "#     weights = []\n",
    "#     biases = []\n",
    "#     last_width=input_dim\n",
    "#     for i,n_units in enumerate(widths):\n",
    "#         W = tf.get_variable(name+'_W'+str(i), shape=[last_width,n_units],\n",
    "#             initializer=tf.contrib.layers.xavier_initializer())\n",
    "#         b = tf.get_variable(name+'_b'+str(i), shape=[n_units],\n",
    "#             initializer=tf.constant_initializer(0.0))\n",
    "#         input = tf.matmul(input, W) + b\n",
    "#         if activation is not None:\n",
    "#             input = activation(input)\n",
    "#         last_width = n_units\n",
    "#         weights.append(W)\n",
    "#         biases.append(b)\n",
    "#     W = tf.get_variable(name+'_W'+str(len(widths)), shape=[last_width,latent_dim],\n",
    "#         initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b = tf.get_variable(name+'_b'+str(len(widths)), shape=[latent_dim],\n",
    "#         initializer=tf.constant_initializer(0.0))\n",
    "#     input = tf.matmul(input,W) + b\n",
    "#     weights.append(W)\n",
    "#     biases.append(b)\n",
    "#     return input, weights, biases\n",
    "\n",
    "\n",
    "# def decoder(input, input_dim, latent_dim, widths, activation, name):\n",
    "#     \"\"\"\n",
    "#     Construct the decoder.\n",
    "\n",
    "#     Arguments:\n",
    "#         input - 2D tensorflow array, input to the network (shape is [?,latent_dim])\n",
    "#         input_dim - Integer, number of state variables in the original data space\n",
    "#         latent_dim - Integer, number of state variables in the decoder space\n",
    "#         widths - List of integers representing how many units are in each network layer\n",
    "#         activation - Tensorflow function to be used as the activation function at each layer\n",
    "#         name - String, prefix to be used in naming the tensorflow variables\n",
    "\n",
    "#     Returns:\n",
    "#         input - Tensorflow array, output of the decoder (shape is [?,input_dim])\n",
    "#         weights - List of tensorflow arrays containing the network weights\n",
    "#         biases - List of tensorflow arrays containing the network biases\n",
    "#     \"\"\"\n",
    "#     weights = []\n",
    "#     biases = []\n",
    "#     last_width=latent_dim\n",
    "#     for i,n_units in enumerate(widths):\n",
    "#         W = tf.get_variable(name+'_W'+str(i), shape=[last_width,n_units],\n",
    "#             initializer=tf.contrib.layers.xavier_initializer())\n",
    "#         b = tf.get_variable(name+'_b'+str(i), shape=[n_units],\n",
    "#             initializer=tf.constant_initializer(0.0))\n",
    "#         input = tf.matmul(input, W) + b\n",
    "#         if activation is not None:\n",
    "#             input = activation(input)\n",
    "#         last_width = n_units\n",
    "#         weights.append(W)\n",
    "#         biases.append(b)\n",
    "#     W = tf.get_variable(name+'_W'+str(len(widths)), shape=[last_width,input_dim],\n",
    "#         initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b = tf.get_variable(name+'_b'+str(len(widths)), shape=[input_dim],\n",
    "#         initializer=tf.constant_initializer(0.0))\n",
    "#     input = tf.matmul(input,W) + b\n",
    "#     weights.append(W)\n",
    "#     biases.append(b)\n",
    "#     return input, weights, biases\n",
    "\n",
    "\n",
    "def sindy_library_tf(z, latent_dim, poly_order, include_sine=False):\n",
    "    \"\"\"\n",
    "    Build the SINDy library.\n",
    "\n",
    "    Arguments:\n",
    "        z - 2D tensorflow array of the snapshots on which to build the library. Shape is number of\n",
    "        time points by the number of state variables.\n",
    "        latent_dim - Integer, number of state variable in z.\n",
    "        poly_order - Integer, polynomial order to which to build the library. Max value is 5.\n",
    "        include_sine - Boolean, whether or not to include sine terms in the library. Default False.\n",
    "\n",
    "    Returns:\n",
    "        2D tensorflow array containing the constructed library. Shape is number of time points by\n",
    "        number of library functions. The number of library functions is determined by the number\n",
    "        of state variables of the input, the polynomial order, and whether or not sines are included.\n",
    "    \"\"\"\n",
    "    library = [tf.ones(tf.shape(z)[0])]\n",
    "\n",
    "    for i in range(latent_dim):\n",
    "        library.append(z[:,i])\n",
    "\n",
    "    if poly_order > 1:\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                library.append(tf.multiply(z[:,i], z[:,j]))\n",
    "\n",
    "    if poly_order > 2:\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    library.append(z[:,i]*z[:,j]*z[:,k])\n",
    "\n",
    "    if poly_order > 3:\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    for p in range(k,latent_dim):\n",
    "                        library.append(z[:,i]*z[:,j]*z[:,k]*z[:,p])\n",
    "\n",
    "    if poly_order > 4:\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    for p in range(k,latent_dim):\n",
    "                        for q in range(p,latent_dim):\n",
    "                            library.append(z[:,i]*z[:,j]*z[:,k]*z[:,p]*z[:,q])\n",
    "\n",
    "    if include_sine:\n",
    "        for i in range(latent_dim):\n",
    "            library.append(tf.sin(z[:,i]))\n",
    "\n",
    "    return tf.stack(library, axis=1)\n",
    "\n",
    "\n",
    "def sindy_library_tf_order2(z, dz, latent_dim, poly_order, include_sine=False):\n",
    "    \"\"\"\n",
    "    Build the SINDy library for a second order system. This is essentially the same as for a first\n",
    "    order system, but library terms are also built for the derivatives.\n",
    "    \"\"\"\n",
    "    library = [tf.ones(tf.shape(z)[0])]\n",
    "\n",
    "    z_combined = tf.concat([z, dz], 1)\n",
    "\n",
    "    for i in range(2*latent_dim):\n",
    "        library.append(z_combined[:,i])\n",
    "\n",
    "    if poly_order > 1:\n",
    "        for i in range(2*latent_dim):\n",
    "            for j in range(i,2*latent_dim):\n",
    "                library.append(tf.multiply(z_combined[:,i], z_combined[:,j]))\n",
    "\n",
    "    if poly_order > 2:\n",
    "        for i in range(2*latent_dim):\n",
    "            for j in range(i,2*latent_dim):\n",
    "                for k in range(j,2*latent_dim):\n",
    "                    library.append(z_combined[:,i]*z_combined[:,j]*z_combined[:,k])\n",
    "\n",
    "    if poly_order > 3:\n",
    "        for i in range(2*latent_dim):\n",
    "            for j in range(i,2*latent_dim):\n",
    "                for k in range(j,2*latent_dim):\n",
    "                    for p in range(k,2*latent_dim):\n",
    "                        library.append(z_combined[:,i]*z_combined[:,j]*z_combined[:,k]*z_combined[:,p])\n",
    "\n",
    "    if poly_order > 4:\n",
    "        for i in range(2*latent_dim):\n",
    "            for j in range(i,2*latent_dim):\n",
    "                for k in range(j,2*latent_dim):\n",
    "                    for p in range(k,2*latent_dim):\n",
    "                        for q in range(p,2*latent_dim):\n",
    "                            library.append(z_combined[:,i]*z_combined[:,j]*z_combined[:,k]*z_combined[:,p]*z_combined[:,q])\n",
    "\n",
    "    if include_sine:\n",
    "        for i in range(2*latent_dim):\n",
    "            library.append(tf.sin(z_combined[:,i]))\n",
    "\n",
    "    return tf.stack(library, axis=1)\n",
    "\n",
    "\n",
    "def z_derivative(input, dx, weights, biases, activation='elu'):\n",
    "    \"\"\"\n",
    "    Compute the first order time derivatives by propagating through the network.\n",
    "\n",
    "    Arguments:\n",
    "        input - 2D tensorflow array, input to the network. Dimensions are number of time points\n",
    "        by number of state variables.\n",
    "        dx - First order time derivatives of the input to the network.\n",
    "        weights - List of tensorflow arrays containing the network weights\n",
    "        biases - List of tensorflow arrays containing the network biases\n",
    "        activation - String specifying which activation function to use. Options are\n",
    "        'elu' (exponential linear unit), 'relu' (rectified linear unit), 'sigmoid',\n",
    "        or linear.\n",
    "\n",
    "    Returns:\n",
    "        dz - Tensorflow array, first order time derivatives of the network output.\n",
    "    \"\"\"\n",
    "    dz = dx\n",
    "    if activation == 'elu':\n",
    "        for i in range(len(weights)-1):\n",
    "            input = tf.matmul(input, weights[i]) + biases[i]\n",
    "            dz = tf.multiply(tf.minimum(tf.exp(input),1.0),\n",
    "                                  tf.matmul(dz, weights[i]))\n",
    "            input = tf.nn.elu(input)\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "    elif activation == 'relu':\n",
    "        for i in range(len(weights)-1):\n",
    "            input = tf.matmul(input, weights[i]) + biases[i]\n",
    "            dz = tf.multiply(tf.to_float(input>0), tf.matmul(dz, weights[i]))\n",
    "            input = tf.nn.relu(input)\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "    elif activation == 'sigmoid':\n",
    "        for i in range(len(weights)-1):\n",
    "            input = tf.matmul(input, weights[i]) + biases[i]\n",
    "            input = tf.sigmoid(input)\n",
    "            dz = tf.multiply(tf.multiply(input, 1-input), tf.matmul(dz, weights[i]))\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "    else:\n",
    "        for i in range(len(weights)-1):\n",
    "            dz = tf.matmul(dz, weights[i])\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "    return dz\n",
    "\n",
    "\n",
    "def z_derivative_order2(input, dx, ddx, weights, biases, activation='elu'):\n",
    "    \"\"\"\n",
    "    Compute the first and second order time derivatives by propagating through the network.\n",
    "\n",
    "    Arguments:\n",
    "        input - 2D tensorflow array, input to the network. Dimensions are number of time points\n",
    "        by number of state variables.\n",
    "        dx - First order time derivatives of the input to the network.\n",
    "        ddx - Second order time derivatives of the input to the network.\n",
    "        weights - List of tensorflow arrays containing the network weights\n",
    "        biases - List of tensorflow arrays containing the network biases\n",
    "        activation - String specifying which activation function to use. Options are\n",
    "        'elu' (exponential linear unit), 'relu' (rectified linear unit), 'sigmoid',\n",
    "        or linear.\n",
    "\n",
    "    Returns:\n",
    "        dz - Tensorflow array, first order time derivatives of the network output.\n",
    "        ddz - Tensorflow array, second order time derivatives of the network output.\n",
    "    \"\"\"\n",
    "    dz = dx\n",
    "    ddz = ddx\n",
    "    if activation == 'elu':\n",
    "        for i in range(len(weights)-1):\n",
    "            input = tf.matmul(input, weights[i]) + biases[i]\n",
    "            dz_prev = tf.matmul(dz, weights[i])\n",
    "            elu_derivative = tf.minimum(tf.exp(input),1.0)\n",
    "            elu_derivative2 = tf.multiply(tf.exp(input), tf.to_float(input<0))\n",
    "            dz = tf.multiply(elu_derivative, dz_prev)\n",
    "            ddz = tf.multiply(elu_derivative2, tf.square(dz_prev)) \\\n",
    "                  + tf.multiply(elu_derivative, tf.matmul(ddz, weights[i]))\n",
    "            input = tf.nn.elu(input)\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "        ddz = tf.matmul(ddz, weights[-1])\n",
    "    elif activation == 'relu':\n",
    "        # NOTE: currently having trouble assessing accuracy of 2nd derivative due to discontinuity\n",
    "        for i in range(len(weights)-1):\n",
    "            input = tf.matmul(input, weights[i]) + biases[i]\n",
    "            relu_derivative = tf.to_float(input>0)\n",
    "            dz = tf.multiply(relu_derivative, tf.matmul(dz, weights[i]))\n",
    "            ddz = tf.multiply(relu_derivative, tf.matmul(ddz, weights[i]))\n",
    "            input = tf.nn.relu(input)\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "        ddz = tf.matmul(ddz, weights[-1])\n",
    "    elif activation == 'sigmoid':\n",
    "        for i in range(len(weights)-1):\n",
    "            input = tf.matmul(input, weights[i]) + biases[i]\n",
    "            input = tf.sigmoid(input)\n",
    "            dz_prev = tf.matmul(dz, weights[i])\n",
    "            sigmoid_derivative = tf.multiply(input, 1-input)\n",
    "            sigmoid_derivative2 = tf.multiply(sigmoid_derivative, 1 - 2*input)\n",
    "            dz = tf.multiply(sigmoid_derivative, dz_prev)\n",
    "            ddz = tf.multiply(sigmoid_derivative2, tf.square(dz_prev)) \\\n",
    "                  + tf.multiply(sigmoid_derivative, tf.matmul(ddz, weights[i]))\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "        ddz = tf.matmul(ddz, weights[-1])\n",
    "    else:\n",
    "        for i in range(len(weights)-1):\n",
    "            dz = tf.matmul(dz, weights[i])\n",
    "            ddz = tf.matmul(ddz, weights[i])\n",
    "        dz = tf.matmul(dz, weights[-1])\n",
    "        ddz = tf.matmul(ddz, weights[-1])\n",
    "    return dz,ddz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import full_network, define_loss\n",
    "\n",
    "\n",
    "def train_network(training_data, val_data, params):\n",
    "    # SET UP NETWORK\n",
    "    autoencoder_network = full_network(params)\n",
    "    loss, losses, loss_refinement = define_loss(autoencoder_network, params)\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    train_op_refinement = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_refinement)\n",
    "    saver = tf.train.Saver(var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "\n",
    "    validation_dict = create_feed_dictionary(val_data, params, idxs=None)\n",
    "\n",
    "    x_norm = np.mean(val_data['x']**2)\n",
    "    if params['model_order'] == 1:\n",
    "        sindy_predict_norm_x = np.mean(val_data['dx']**2)\n",
    "    else:\n",
    "        sindy_predict_norm_x = np.mean(val_data['ddx']**2)\n",
    "\n",
    "    validation_losses = []\n",
    "    sindy_model_terms = [np.sum(params['coefficient_mask'])]\n",
    "\n",
    "    print('TRAINING')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(params['max_epochs']):\n",
    "            for j in range(params['epoch_size']//params['batch_size']):\n",
    "                batch_idxs = np.arange(j*params['batch_size'], (j+1)*params['batch_size'])\n",
    "                train_dict = create_feed_dictionary(training_data, params, idxs=batch_idxs)\n",
    "                sess.run(train_op, feed_dict=train_dict)\n",
    "            \n",
    "            if params['print_progress'] and (i % params['print_frequency'] == 0):\n",
    "                validation_losses.append(print_progress(sess, i, loss, losses, train_dict, validation_dict, x_norm, sindy_predict_norm_x))\n",
    "\n",
    "            if params['sequential_thresholding'] and (i % params['threshold_frequency'] == 0) and (i > 0):\n",
    "                params['coefficient_mask'] = np.abs(sess.run(autoencoder_network['sindy_coefficients'])) > params['coefficient_threshold']\n",
    "                validation_dict['coefficient_mask:0'] = params['coefficient_mask']\n",
    "                print('THRESHOLDING: %d active coefficients' % np.sum(params['coefficient_mask']))\n",
    "                sindy_model_terms.append(np.sum(params['coefficient_mask']))\n",
    "\n",
    "        print('REFINEMENT')\n",
    "        for i_refinement in range(params['refinement_epochs']):\n",
    "            for j in range(params['epoch_size']//params['batch_size']):\n",
    "                batch_idxs = np.arange(j*params['batch_size'], (j+1)*params['batch_size'])\n",
    "                train_dict = create_feed_dictionary(training_data, params, idxs=batch_idxs)\n",
    "                sess.run(train_op_refinement, feed_dict=train_dict)\n",
    "            \n",
    "            if params['print_progress'] and (i_refinement % params['print_frequency'] == 0):\n",
    "                validation_losses.append(print_progress(sess, i_refinement, loss_refinement, losses, train_dict, validation_dict, x_norm, sindy_predict_norm_x))\n",
    "\n",
    "        saver.save(sess, params['data_path'] + params['save_name'])\n",
    "        pickle.dump(params, open(params['data_path'] + params['save_name'] + '_params.pkl', 'wb'))\n",
    "        final_losses = sess.run((losses['decoder'], losses['sindy_x'], losses['sindy_z'],\n",
    "                                 losses['sindy_regularization']),\n",
    "                                feed_dict=validation_dict)\n",
    "        if params['model_order'] == 1:\n",
    "            sindy_predict_norm_z = np.mean(sess.run(autoencoder_network['dz'], feed_dict=validation_dict)**2)\n",
    "        else:\n",
    "            sindy_predict_norm_z = np.mean(sess.run(autoencoder_network['ddz'], feed_dict=validation_dict)**2)\n",
    "        sindy_coefficients = sess.run(autoencoder_network['sindy_coefficients'], feed_dict={})\n",
    "\n",
    "        results_dict = {}\n",
    "        results_dict['num_epochs'] = i\n",
    "        results_dict['x_norm'] = x_norm\n",
    "        results_dict['sindy_predict_norm_x'] = sindy_predict_norm_x\n",
    "        results_dict['sindy_predict_norm_z'] = sindy_predict_norm_z\n",
    "        results_dict['sindy_coefficients'] = sindy_coefficients\n",
    "        results_dict['loss_decoder'] = final_losses[0]\n",
    "        results_dict['loss_decoder_sindy'] = final_losses[1]\n",
    "        results_dict['loss_sindy'] = final_losses[2]\n",
    "        results_dict['loss_sindy_regularization'] = final_losses[3]\n",
    "        results_dict['validation_losses'] = np.array(validation_losses)\n",
    "        results_dict['sindy_model_terms'] = np.array(sindy_model_terms)\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "\n",
    "def print_progress(sess, i, loss, losses, train_dict, validation_dict, x_norm, sindy_predict_norm):\n",
    "    \"\"\"\n",
    "    Print loss function values to keep track of the training progress.\n",
    "\n",
    "    Arguments:\n",
    "        sess - the tensorflow session\n",
    "        i - the training iteration\n",
    "        loss - tensorflow object representing the total loss function used in training\n",
    "        losses - tuple of the individual losses that make up the total loss\n",
    "        train_dict - feed dictionary of training data\n",
    "        validation_dict - feed dictionary of validation data\n",
    "        x_norm - float, the mean square value of the input\n",
    "        sindy_predict_norm - float, the mean square value of the time derivatives of the input.\n",
    "        Can be first or second order time derivatives depending on the model order.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of losses calculated on the validation set.\n",
    "    \"\"\"\n",
    "    training_loss_vals = sess.run((loss,) + tuple(losses.values()), feed_dict=train_dict)\n",
    "    validation_loss_vals = sess.run((loss,) + tuple(losses.values()), feed_dict=validation_dict)\n",
    "    print(\"Epoch %d\" % i)\n",
    "    print(\"   training loss {0}, {1}\".format(training_loss_vals[0],\n",
    "                                             training_loss_vals[1:]))\n",
    "    print(\"   validation loss {0}, {1}\".format(validation_loss_vals[0],\n",
    "                                               validation_loss_vals[1:]))\n",
    "    decoder_losses = sess.run((losses['decoder'], losses['sindy_x']), feed_dict=validation_dict)\n",
    "    loss_ratios = (decoder_losses[0]/x_norm, decoder_losses[1]/sindy_predict_norm)\n",
    "    print(\"decoder loss ratio: %f, decoder SINDy loss  ratio: %f\" % loss_ratios)\n",
    "    return validation_loss_vals\n",
    "\n",
    "\n",
    "def create_feed_dictionary(data, params, idxs=None):\n",
    "    \"\"\"\n",
    "    Create the feed dictionary for passing into tensorflow.\n",
    "\n",
    "    Arguments:\n",
    "        data - Dictionary object containing the data to be passed in. Must contain input data x,\n",
    "        along the first (and possibly second) order time derivatives dx (ddx).\n",
    "        params - Dictionary object containing model and training parameters. The relevant\n",
    "        parameters are model_order (which determines whether the SINDy model predicts first or\n",
    "        second order time derivatives), sequential_thresholding (which indicates whether or not\n",
    "        coefficient thresholding is performed), coefficient_mask (optional if sequential\n",
    "        thresholding is performed; 0/1 mask that selects the relevant coefficients in the SINDy\n",
    "        model), and learning rate (float that determines the learning rate).\n",
    "        idxs - Optional array of indices that selects which examples from the dataset are passed\n",
    "        in to tensorflow. If None, all examples are used.\n",
    "\n",
    "    Returns:\n",
    "        feed_dict - Dictionary object containing the relevant data to pass to tensorflow.\n",
    "    \"\"\"\n",
    "    if idxs is None:\n",
    "        idxs = np.arange(data['x'].shape[0])\n",
    "    feed_dict = {}\n",
    "    feed_dict['x:0'] = data['x'][idxs]\n",
    "    feed_dict['dx:0'] = data['dx'][idxs]\n",
    "    if params['model_order'] == 2:\n",
    "        feed_dict['ddx:0'] = data['ddx'][idxs]\n",
    "    if params['sequential_thresholding']:\n",
    "        feed_dict['coefficient_mask:0'] = params['coefficient_mask']\n",
    "    feed_dict['learning_rate:0'] = params['learning_rate']\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38782e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
